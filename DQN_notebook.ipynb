{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCWxJ6I8GJ1WZ70IUUwyGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yichenghuang980/DQN_algorithm_toy/blob/master/DQN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJqkojr7T3bJ",
        "outputId": "83e947cf-5f0b-43f2-f45a-55dc227b6107"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy5dEfaDss_Z"
      },
      "source": [
        "import argparse\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "import os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal, Categorical\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLXlovRztxRK",
        "outputId": "0cb16264-d077-41db-beac-7aa27d62270d"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opc_Rv-HToVT"
      },
      "source": [
        "# Hyper-parameters\n",
        "seed = 1\n",
        "render = False\n",
        "num_episodes = 500\n",
        "env = gym.make('MountainCar-v0').unwrapped\n",
        "num_state = env.observation_space.shape[0]\n",
        "num_action = env.action_space.n\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1zEXhK1s90G"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_state, 100)\n",
        "        self.fc2 = nn.Linear(100, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        action_prob = self.fc2(x)\n",
        "        return action_prob"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83MmezYyUzu1"
      },
      "source": [
        "class DQN():\n",
        "\n",
        "    capacity = 8000\n",
        "    learning_rate = 1e-3\n",
        "    memory_count = 0\n",
        "    batch_size = 256\n",
        "    gamma = 0.995\n",
        "    update_count = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.target_net, self.act_net = Net(), Net()\n",
        "        self.memory = [None]*self.capacity\n",
        "        self.optimizer = optim.Adam(self.act_net.parameters(), self.learning_rate)\n",
        "        self.loss_func = nn.MSELoss()\n",
        "        self.writer = SummaryWriter('./DQN/logs')\n",
        "        self.cost_his = []\n",
        "\n",
        "    def select_action(self,state):\n",
        "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
        "        value = self.act_net(state)\n",
        "        action_max_value, index = torch.max(value, 1)\n",
        "        action = index.item()\n",
        "        if np.random.rand(1) >= 0.9: # epslion greedy\n",
        "            action = np.random.choice(range(num_action), 1).item()\n",
        "        return action\n",
        "\n",
        "    def store_transition(self,transition):\n",
        "        index = self.memory_count % self.capacity\n",
        "        self.memory[index] = transition\n",
        "        self.memory_count += 1\n",
        "        return self.memory_count >= self.capacity\n",
        "\n",
        "    def update(self):\n",
        "        if self.memory_count >= self.capacity:\n",
        "            state = torch.tensor([t.state for t in self.memory]).float()\n",
        "            action = torch.LongTensor([t.action for t in self.memory]).view(-1,1).long()\n",
        "            reward = torch.tensor([t.reward for t in self.memory]).float()\n",
        "            next_state = torch.tensor([t.next_state for t in self.memory]).float()\n",
        "\n",
        "            reward = (reward - reward.mean()) / (reward.std() + 1e-7)\n",
        "            with torch.no_grad():\n",
        "                target_v = reward + self.gamma * self.target_net(next_state).max(1)[0]\n",
        "\n",
        "            #Update...\n",
        "            for index in BatchSampler(SubsetRandomSampler(range(len(self.memory))), batch_size=self.batch_size, drop_last=False):\n",
        "                v = (self.act_net(state).gather(1, action))[index]\n",
        "                loss = self.loss_func(target_v[index].unsqueeze(1), (self.act_net(state).gather(1, action))[index])\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.writer.add_scalar('loss/value_loss', loss, self.update_count)\n",
        "                self.cost_his.append(loss)\n",
        "                self.update_count +=1\n",
        "                if self.update_count % 100 ==0:\n",
        "                    self.target_net.load_state_dict(self.act_net.state_dict())\n",
        "        else:\n",
        "            print(\"Memory Buff is too less\")\n",
        "    \n",
        "    \n",
        "    def plot_cost(self):\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
        "        plt.ylabel('Cost')\n",
        "        plt.xlabel('training steps')\n",
        "        plt.show()\n",
        "        \n",
        "    \n",
        "def main():\n",
        "\n",
        "    agent = DQN()\n",
        "    for i_ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if render: env.render()\n",
        "        for t in range(10000):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            if render: env.render()\n",
        "            transition = Transition(state, action, reward, next_state)\n",
        "            agent.store_transition(transition)\n",
        "            state = next_state\n",
        "            if done or t >=9999:\n",
        "                agent.writer.add_scalar('live/finish_step', t+1, global_step=i_ep)\n",
        "                agent.update()\n",
        "                if i_ep % 10 == 0:\n",
        "                    print(\"episodes {}, step is {} \".format(i_ep, t))\n",
        "                break\n",
        "    agent.plot_cost()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU9OKYX6VdO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250978a8-1cb9-4d7d-f217-d64955f88446"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episodes 0, step is 9999 \n",
            "episodes 10, step is 248 \n",
            "episodes 20, step is 176 \n",
            "episodes 30, step is 234 \n",
            "episodes 40, step is 9999 \n",
            "episodes 50, step is 9999 \n",
            "episodes 60, step is 9999 \n",
            "episodes 70, step is 9999 \n",
            "episodes 80, step is 9999 \n",
            "episodes 90, step is 9999 \n",
            "episodes 100, step is 9999 \n",
            "episodes 110, step is 9999 \n",
            "episodes 120, step is 9999 \n",
            "episodes 130, step is 9999 \n",
            "episodes 140, step is 9999 \n",
            "episodes 150, step is 9999 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYQ2ipTVQ7l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}